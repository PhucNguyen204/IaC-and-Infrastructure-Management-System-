# PhÃ¢n tÃ­ch Kafka vÃ  Redis trong Dá»± Ã¡n IaaS Platform

## ðŸ“Š Tá»•ng quan Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IaaS Platform Architecture                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Auth Service â”‚     â”‚ Provisioning â”‚     â”‚  Monitoring  â”‚
â”‚   (8082)     â”‚â”€â”€â”€â”€â–¶â”‚   Service    â”‚â”€â”€â”€â”€â–¶â”‚   Service    â”‚
â”‚              â”‚     â”‚    (8083)    â”‚     â”‚   (8084)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                     â”‚
       â”‚ Redis              â”‚ Kafka               â”‚ Kafka
       â”‚ (Session)          â”‚ (Events)            â”‚ (Consumer)
       â”‚                    â”‚                     â”‚
       â–¼                    â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Redis     â”‚     â”‚    Kafka     â”‚     â”‚ Elasticsearchâ”‚
â”‚   (6379)     â”‚â—€â”€â”€â”€â”€â”‚   (9092)     â”‚â”€â”€â”€â”€â–¶â”‚   (9200)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                    â–²
       â”‚                    â”‚
       â”‚ Cache              â”‚ Events
       â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Events    â”‚   Infrastructure     â”‚
â”‚      (Container Lifecycle)   (State Changes)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”´ REDIS - Distributed Cache & Session Store

### 1. Vai trÃ² chÃ­nh

Redis Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m:
- **Session Storage** - LÆ°u trá»¯ refresh tokens
- **Cache Layer** - Cache thÃ´ng tin cluster Ä‘á»ƒ giáº£m táº£i database
- **Fast Read/Write** - Key-value store nhanh chÃ³ng

### 2. Sá»­ dá»¥ng trong Authentication Service

**File:** `vcs-authentication-service/usecases/services/auth.go`

#### a) LÆ°u Refresh Token
```go
// Khi user login thÃ nh cÃ´ng
func (s *authService) Login(ctx context.Context, username, password string) (string, string, error) {
    // ... validate user ...
    
    accessToken, _ := s.generateAccessToken(user.ID, scopes)
    refreshToken, _ := s.generateRefreshToken()
    
    // LÆ¯U refresh token vÃ o Redis vá»›i TTL 7 ngÃ y
    s.redisClient.Set(ctx, "refresh:"+refreshToken, user.ID, time.Hour*24*7)
    
    return accessToken, refreshToken, nil
}
```

**Key Pattern:** `refresh:{token}` â†’ `user_id`  
**TTL:** 7 ngÃ y  
**Purpose:** Cho phÃ©p renew access token mÃ  khÃ´ng cáº§n login láº¡i

#### b) Refresh Access Token
```go
func (s *authService) RefreshAccessToken(ctx context.Context, refreshToken string) (string, error) {
    // Äá»ŒC user_id tá»« Redis
    userId, err := s.redisClient.Get(ctx, "refresh:"+refreshToken)
    if err != nil {
        return "", err // Token expired hoáº·c khÃ´ng tá»“n táº¡i
    }
    
    // Generate access token má»›i
    accessToken, _ := s.generateAccessToken(userId, scopes)
    return accessToken, nil
}
```

#### c) Invalidate Token khi Ä‘á»•i password
```go
func (s *authService) UpdatePassword(ctx context.Context, userId, currentPassword, newPassword string) error {
    // ... update password ...
    
    // XÃ“A refresh token khá»i Redis
    s.redisClient.Del(ctx, "refresh:"+user.ID)
    
    return nil
}
```

**Táº¡i sao dÃ¹ng Redis?**
- âš¡ **Fast lookup** - O(1) time complexity
- â° **Auto expiration** - TTL tá»± Ä‘á»™ng xÃ³a token cÅ©
- ðŸ”’ **Secure** - Token khÃ´ng lÆ°u trong database chÃ­nh
- ðŸ“ˆ **Scalable** - Dá»… dÃ ng scale horizontal

---

### 3. Sá»­ dá»¥ng trong Provisioning Service

**File:** `vcs-infrastructure-provisioning-service/usecases/services/cache_service.go`

#### a) Cache Cluster Information
```go
type ICacheService interface {
    GetClusterInfo(ctx context.Context, clusterID string) (*dto.ClusterInfoResponse, bool)
    SetClusterInfo(ctx context.Context, clusterID string, info *dto.ClusterInfoResponse, ttl time.Duration) error
    InvalidateCluster(ctx context.Context, clusterID string) error
}
```

**Cache Keys:**
- `cluster:info:{cluster_id}` - ThÃ´ng tin cluster (nodes, endpoints, status)
- `cluster:stats:{cluster_id}` - Sá»‘ liá»‡u thá»‘ng kÃª (CPU, RAM, connections)
- `cluster:replication:{cluster_id}` - Replication status (primary, replicas, lag)

#### b) Flow sá»­ dá»¥ng Cache

```go
// 1. Äá»c tá»« cache trÆ°á»›c
func (s *Service) GetClusterInfo(clusterID string) (*ClusterInfo, error) {
    // Check cache
    if cached, found := s.cache.GetClusterInfo(ctx, clusterID); found {
        return cached, nil  // âœ… Cache HIT - nhanh!
    }
    
    // Cache MISS - query database
    info := s.queryDatabase(clusterID)
    
    // LÆ°u vÃ o cache vá»›i TTL 5 phÃºt
    s.cache.SetClusterInfo(ctx, clusterID, info, 5*time.Minute)
    
    return info, nil
}
```

#### c) Cache Invalidation
```go
// Khi cluster thay Ä‘á»•i â†’ xÃ³a cache
func (s *cacheService) InvalidateCluster(ctx context.Context, clusterID string) error {
    keys := []string{
        fmt.Sprintf("cluster:info:%s", clusterID),
        fmt.Sprintf("cluster:stats:%s", clusterID),
        fmt.Sprintf("cluster:replication:%s", clusterID),
    }
    return s.redis.Del(ctx, keys...).Err()
}
```

**Khi nÃ o invalidate cache?**
- âœ… Cluster start/stop/restart
- âœ… Node thÃªm/xÃ³a
- âœ… Configuration thay Ä‘á»•i
- âœ… Failover xáº£y ra

**Performance Improvement:**
```
Without Cache:  
  Database Query: ~50-200ms
  Complex JOIN: ~100-500ms

With Redis Cache:
  Cache Hit: ~1-5ms (100x faster!)
  TTL: 5 minutes
  Cache Hit Rate: ~80-90%
```

---

## ðŸŸ¢ KAFKA - Event Streaming Platform

### 1. Vai trÃ² chÃ­nh

Kafka Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m:
- **Event Bus** - Truyá»n táº£i events giá»¯a cÃ¡c services
- **Decoupling** - TÃ¡ch biá»‡t services (loose coupling)
- **Async Processing** - Xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™
- **Event Sourcing** - LÆ°u láº¡i lá»‹ch sá»­ cÃ¡c sá»± kiá»‡n

### 2. Architecture Kafka trong Dá»± Ã¡n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Kafka Architecture                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Docker Events          User Actions
     â”‚                      â”‚
     â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Provisioning Service (Producer)   â”‚
â”‚   - Publish infrastructure events   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Kafka     â”‚  Topic: infrastructure.events
      â”‚  (Broker)   â”‚  Partitions: Auto
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  Replication: 1
             â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Monitoring Service    â”‚      â”‚  Provisioning Service    â”‚
â”‚  (Consumer)            â”‚      â”‚  (Consumer)              â”‚
â”‚  â†’ Elasticsearch       â”‚      â”‚  â†’ Cache Invalidation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Producer - Provisioning Service

**File:** `vcs-infrastructure-provisioning-service/infrastructures/kafka/producer.go`

#### a) Kafka Producer Configuration
```go
type kafkaProducer struct {
    writer *kafka.Writer
}

func NewKafkaProducer(env env.KafkaEnv, logger logger.ILogger) IKafkaProducer {
    writer := &kafka.Writer{
        Addr:                   kafka.TCP(env.Brokers...),
        Topic:                  env.Topic,               // infrastructure.events
        Balancer:               &kafka.Hash{},           // Hash by key
        AllowAutoTopicCreation: true,                    // Auto create topic
        Async:                  true,                    // âš¡ Async publish
        BatchSize:              100,                     // Batch 100 messages
        BatchTimeout:           10 * time.Millisecond,   // hoáº·c 10ms
        RequiredAcks:           1,                       // Leader ack
        MaxAttempts:            3,                       // Retry 3 láº§n
        Compression:            kafka.Snappy,            // NÃ©n dá»¯ liá»‡u
    }
    return &kafkaProducer{writer: writer}
}
```

**Táº¡i sao Async = true?**
- âš¡ **Non-blocking** - KhÃ´ng Ä‘á»£i Kafka confirm
- ðŸš€ **High throughput** - CÃ³ thá»ƒ publish hÃ ng ngÃ n events/giÃ¢y
- ðŸ“¦ **Batching** - Gom nhiá»u messages láº¡i gá»­i 1 láº§n

#### b) Event Structure
```go
type InfrastructureEvent struct {
    InstanceID string                 // Cluster/Instance ID
    UserID     string                 // User thá»±c hiá»‡n action
    Type       string                 // "postgres", "nginx", "k8s"
    Action     string                 // "created", "started", "stopped", "deleted"
    Timestamp  time.Time              // Thá»i gian event
    Metadata   map[string]interface{} // ThÃ´ng tin bá»• sung
}
```

#### c) Publish Event
```go
func (kp *kafkaProducer) PublishEvent(ctx context.Context, event InfrastructureEvent) error {
    event.Timestamp = time.Now()
    
    eventBytes, _ := json.Marshal(event)
    
    msg := kafka.Message{
        Key:   []byte(event.InstanceID),  // Key = instance_id (same key â†’ same partition)
        Value: eventBytes,
        Time:  event.Timestamp,
    }
    
    // Gá»­i Ä‘áº¿n Kafka (async)
    if err := kp.writer.WriteMessages(ctx, msg); err != nil {
        return err
    }
    
    return nil
}
```

### 4. Docker Event Listener â†’ Kafka

**File:** `vcs-infrastructure-provisioning-service/usecases/services/docker_event_listener_service.go`

**Flow hoáº¡t Ä‘á»™ng:**

```
1. Docker Engine phÃ¡t ra event (container start/stop/die)
        â†“
2. Docker Event Listener Service láº¯ng nghe
        â†“
3. Parse event â†’ Cáº­p nháº­t database status
        â†“
4. Publish event lÃªn Kafka
        â†“
5. Kafka broadcast Ä‘áº¿n cÃ¡c consumers
```

#### Code:
```go
func (s *dockerEventListenerService) handleEvent(ctx context.Context, event events.Message) {
    containerID := event.ID
    action := string(event.Action)
    
    // Map Docker action â†’ Infrastructure status
    var status entities.InfrastructureStatus
    switch event.Action {
    case events.ActionStart:
        status = entities.StatusRunning
    case events.ActionStop, events.ActionDie:
        status = entities.StatusStopped
    case events.ActionDestroy:
        status = entities.StatusDeleted
    }
    
    // Cáº­p nháº­t database
    infra, _ := s.infraRepo.FindByContainerID(ctx, containerID)
    infra.Status = status
    s.infraRepo.Update(infra)
    
    // ðŸ”¥ PUBLISH LÃŠN KAFKA
    kafkaEvent := kafka.InfrastructureEvent{
        InstanceID: infra.ID,
        UserID:     infra.UserID,
        Type:       string(infra.Type),  // "postgres", "nginx"
        Action:     action,               // "start", "stop", "die"
        Timestamp:  time.Now(),
        Metadata: map[string]interface{}{
            "container_id":   containerID,
            "container_name": containerName,
            "status":         string(status),
        },
    }
    
    s.kafkaProducer.PublishEvent(ctx, kafkaEvent)
    
    // Bonus: Broadcast qua WebSocket (real-time UI update)
    if s.wsBroadcaster != nil {
        s.wsBroadcaster.BroadcastUpdate(update)
    }
}
```

**CÃ¡c loáº¡i events Ä‘Æ°á»£c publish:**
- `postgres.created` - PostgreSQL cluster/instance táº¡o má»›i
- `postgres.started` - PostgreSQL start
- `postgres.stopped` - PostgreSQL stop
- `postgres.deleted` - PostgreSQL xÃ³a
- `nginx.created` - Nginx cluster táº¡o má»›i
- `nginx.started` - Nginx start
- ... tÆ°Æ¡ng tá»± cho K8s, Docker services

### 5. Consumer - Monitoring Service

**File:** `vcs-infrastructure-monitoring-service/infrastructures/kafka/consumer.go`

#### a) Kafka Consumer Configuration
```go
func NewKafkaConsumer(env env.KafkaEnv, esClient elasticsearch.IElasticsearchClient) IKafkaConsumer {
    readers := make([]*kafka.Reader, 0)
    
    // Subscribe nhiá»u topics
    topics := []string{
        "postgres.created", "postgres.started", "postgres.stopped",
        "nginx.created", "nginx.started", "nginx.stopped",
        // ... more topics
    }
    
    for _, topic := range topics {
        reader := kafka.NewReader(kafka.ReaderConfig{
            Brokers: env.Brokers,            // kafka:9092
            GroupID: "monitoring-consumer-group",  // Consumer group
            Topic:   topic,
        })
        readers = append(readers, reader)
    }
    
    return &kafkaConsumer{
        readers:  readers,
        esClient: esClient,
    }
}
```

#### b) Consume Messages vÃ  Index vÃ o Elasticsearch
```go
func (kc *kafkaConsumer) consumeMessages(ctx context.Context, reader *kafka.Reader) {
    for {
        // Äá»c message tá»« Kafka
        msg, err := reader.ReadMessage(ctx)
        if err != nil {
            continue
        }
        
        // Parse event
        var event InfrastructureEvent
        json.Unmarshal(msg.Value, &event)
        
        // Táº¡o log entry
        logEntry := elasticsearch.LogEntry{
            InstanceID: event.InstanceID,
            UserID:     event.UserID,
            Type:       event.Type,
            Action:     event.Action,
            Message:    fmt.Sprintf("%s %s", event.Type, event.Action),
            Level:      "info",
            Metadata:   event.Metadata,
        }
        
        // ðŸ“Š INDEX VÃ€O ELASTICSEARCH
        kc.esClient.IndexLog(ctx, logEntry)
    }
}
```

**Flow:**
```
Kafka Event â†’ Monitoring Service â†’ Elasticsearch â†’ Kibana Dashboard
```

### 6. Consumer - Cache Invalidation

**File:** `vcs-infrastructure-provisioning-service/infrastructures/kafka/consumer.go`

```go
func (c *eventConsumer) handleEvent(ctx context.Context, event InfrastructureEvent) error {
    // Khi cÃ³ event thay Ä‘á»•i cluster
    // â†’ Invalidate Redis cache
    
    if event.Action == "started" || 
       event.Action == "stopped" || 
       event.Action == "deleted" {
        
        // XÃ³a cache cá»§a cluster nÃ y
        return c.cache.InvalidateCluster(ctx, event.InstanceID)
    }
    
    return nil
}
```

**Lá»£i Ã­ch:**
- âœ… Cache luÃ´n consistent vá»›i database
- âœ… KhÃ´ng cáº§n manual invalidation
- âœ… Event-driven architecture

---

## ðŸ”„ So sÃ¡nh Redis vs Kafka

| TiÃªu chÃ­ | Redis | Kafka |
|----------|-------|-------|
| **Purpose** | Cache, Session Store | Event Streaming |
| **Data Type** | Key-Value | Message Stream |
| **Persistence** | In-memory (+ RDB/AOF) | Disk (Log) |
| **Speed** | Cá»±c nhanh (~1ms) | Nhanh (~5-10ms) |
| **Retention** | TTL (giÃ¢y â†’ giá») | Days/Weeks/Forever |
| **Pattern** | Request/Response | Pub/Sub |
| **Use Case** | Fast reads, Cache | Event processing, Logging |
| **Scalability** | Vertical | Horizontal (partitions) |

---

## ðŸ“ˆ Metrics & Monitoring

### Redis Metrics cáº§n theo dÃµi:
```bash
# Memory usage
redis-cli INFO memory

# Cache hit rate
hits = GET commands that found the key
misses = GET commands that didn't find the key
hit_rate = hits / (hits + misses)

# Eviction count
redis-cli INFO stats | grep evicted_keys
```

**Target:**
- Hit Rate: > 80%
- Eviction Rate: < 5%
- Memory Usage: < 70%

### Kafka Metrics cáº§n theo dÃµi:
```bash
# Consumer lag (ì–¼ë§ˆë‚˜ cháº­m trá»…)
kafka-consumer-groups --bootstrap-server kafka:9092 \
  --describe --group monitoring-consumer-group

# Messages per second
kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list kafka:9092 \
  --topic infrastructure.events
```

**Target:**
- Consumer Lag: < 100 messages
- Publish Latency: < 10ms
- Processing Time: < 50ms

---

## ðŸŽ¯ Best Practices

### Redis:
1. **Set appropriate TTL** - KhÃ´ng Ä‘á»ƒ cache cÅ© quÃ¡ lÃ¢u
2. **Use pipeline** - Batch multiple commands
3. **Monitor memory** - TrÃ¡nh out of memory
4. **Use connection pool** - TÃ¡i sá»­ dá»¥ng connections
5. **Compression** - NÃ©n data lá»›n trÆ°á»›c khi cache

### Kafka:
1. **Use async producer** - Non-blocking publish
2. **Batch messages** - Giáº£m network overhead
3. **Compression** - Snappy hoáº·c LZ4
4. **Monitor consumer lag** - Äáº£m báº£o consumer ká»‹p xá»­ lÃ½
5. **Partition by key** - Äáº£m báº£o ordering
6. **Set retention policy** - XÃ³a message cÅ© tá»± Ä‘á»™ng

---

## ðŸš€ Káº¿t luáº­n

### Redis trong dá»± Ã¡n:
âœ… **Authentication:** Session storage cho refresh tokens (7 ngÃ y TTL)  
âœ… **Provisioning:** Cache cluster info/stats/replication (5 phÃºt TTL)  
âœ… **Performance:** Giáº£m 80-90% database queries  

### Kafka trong dá»± Ã¡n:
âœ… **Event Bus:** Truyá»n táº£i infrastructure events giá»¯a services  
âœ… **Monitoring:** Stream events vÃ o Elasticsearch Ä‘á»ƒ analytics  
âœ… **Cache Sync:** Trigger cache invalidation khi cÃ³ thay Ä‘á»•i  
âœ… **Decoupling:** Services khÃ´ng phá»¥ thuá»™c trá»±c tiáº¿p vÃ o nhau  

**Architecture Pattern:** Event-Driven Microservices vá»›i Cache Layer

```
User Request â†’ Service â†’ Database
                â†“
            Kafka Event â†’ [Monitoring, Cache Invalidation, Webhooks, ...]
                â†“
            Redis Cache â†’ Fast Response
```

